{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Profiler and Schema Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Profiles given input data based on the custom queries you provide, and validates its schema against schema repository. \n",
    "You can find how to insert a schema to schema-repository in README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark job configuration parameters like memory and cores may vary from one job to other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\"name\":\"data-profiler\", \n",
    " \"executorMemory\": \"2GB\", \n",
    " \"executorCores\": 4, \n",
    " \"conf\": {\"spark.jars.packages\": \"com.databricks:spark-avro_2.11:4.0.0,com.github.gphat:censorinus_2.11:2.1.13\"} \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameters that will be overwritten by values passed externally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "val dataFormat = \"data-format\"\n",
    "val delimiter = \"\"\n",
    "val inputDataLocation = \"input-data-location\"\n",
    "val appName = \"app-name\" \n",
    "val schemaRepoUrl = \"schema-repo-url\"\n",
    "val scheRepoSubjectName = \"subject-name\"\n",
    "val schemaVersionId = \"schema-version\"\n",
    "val customQ1 = \"custom-query-1\"\n",
    "val customQ1ResultThreshold = 0\n",
    "val customQ1Operator = \"custom-operator-1\"\n",
    "val customQ2 = \"custom-query-2\"\n",
    "val customQ2ResultThreshold = 0\n",
    "val customQ2Operator = \"custom-operator-2\"\n",
    "val customQ3 = \"custom-query-3\"\n",
    "val customQ3ResultThreshold = 0\n",
    "val customQ3Operator = \"custom-query-3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup datadog statsd interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import github.gphat.censorinus.DogStatsDClient\n",
    "\n",
    "val statsd = new DogStatsDClient(hostname = \"localhost\", port = 8125, prefix = \"mlp.validator\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data, if data being read is CSV, it needs to have a header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df = dataFormat match {\n",
    "    case \"parquet\" => spark.read.parquet(inputDataLocation)\n",
    "    case \"json\" => spark.read.json(inputDataLocation)\n",
    "    case \"csv\" => spark.read.option(\"mode\", \"DROPMALFORMED\").option(\"header\", \"true\").option(\"delimiter\", delimiter).csv(inputDataLocation)\n",
    "    case _ => throw new Exception(s\"$dataFormat, as a dataformat is not supported \")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publish some basic stats about the data. This can be extended further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val recordCount = df.count()\n",
    "val numColumns = df.columns.size\n",
    "statsd.histogram(name = \"recordCount\", value = recordCount, tags = Seq(s\"appName:$appName\", \"data-validation\", \"env:dev\"));\n",
    "statsd.histogram(name = \"numColumns\", value = numColumns, tags = Seq(s\"appName:$appName\", \"data-validation\",\"env:dev\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read registered schema from schema repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility method to call rest endpoint for schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.io.IOException;\n",
    "\n",
    "import org.apache.http.HttpEntity;\n",
    "import org.apache.http.HttpResponse;\n",
    "import org.apache.http.client.ClientProtocolException;\n",
    "import org.apache.http.client.ResponseHandler;\n",
    "import org.apache.http.client.methods.HttpGet;\n",
    "import org.apache.http.impl.client.CloseableHttpClient;\n",
    "import org.apache.http.impl.client.HttpClients;\n",
    "import org.apache.http.util.EntityUtils;\n",
    "\n",
    "def getSchema(url: String) : String = {\n",
    "    val httpclient: CloseableHttpClient = HttpClients.createDefault()\n",
    "    try {\n",
    "      val httpget: HttpGet = new HttpGet(url)\n",
    "      println(\"Executing request \" + httpget.getRequestLine)\n",
    "      val responseHandler: ResponseHandler[String] =\n",
    "        new ResponseHandler[String]() {\n",
    "          override def handleResponse(response: HttpResponse): String = {\n",
    "            var status: Int = response.getStatusLine.getStatusCode\n",
    "            if (status >= 200 && status < 300) {\n",
    "              var entity: HttpEntity = response.getEntity\n",
    "              if (entity != null) EntityUtils.toString(entity) else null\n",
    "            } else {\n",
    "              throw new ClientProtocolException(\n",
    "                \"Unexpected response status: \" + status);\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "       httpclient.execute(httpget, responseHandler)  \n",
    "    } finally {\n",
    "        httpclient.close()\n",
    "        None\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create url from input parameters and feth schema for specified version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val schema_url = s\"$schemaRepoUrl/schema-repo/$scheRepoSubjectName/id/$schemaVersionId\"\n",
    "val publishedSchema = getSchema(schema_url) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Avro schema registered to Spark SQL Schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import com.databricks.spark.avro._\n",
    "import org.apache.avro.Schema.Parser\n",
    "val schema = new Parser().parse(publishedSchema)\n",
    "\n",
    "import com.databricks.spark.avro.SchemaConverters\n",
    "val structSchema =  SchemaConverters.toSqlType(schema).dataType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility method to traverse schema tree and find the leaf node names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scala.collection.mutable.ListBuffer\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "def findFields(path: String, dt: DataType, columnNames: ListBuffer[String]): Unit = dt match {\n",
    "    case s: StructType =>\n",
    "      s.fields.foreach(f => findFields(path + \".\" + f.name, f.dataType, columnNames))\n",
    "    case s: ArrayType => findFields(path, s.elementType, columnNames)\n",
    "    case other =>\n",
    "      columnNames += path.substring(1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var dfColumnNames = new ListBuffer[String]()\n",
    "findFields(\"\", df.schema, dfColumnNames)\n",
    "\n",
    "print(dfColumnNames.toList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var publishedSchemaDataColumnNames = new ListBuffer[String]()\n",
    "findFields(\"\", structSchema, publishedSchemaDataColumnNames)\n",
    "\n",
    "print(publishedSchemaDataColumnNames.toList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val sourceColumns = dfColumnNames.toSet\n",
    "val publishedColumns = publishedSchemaDataColumnNames.toSet\n",
    "val differenceColumns = publishedColumns.diff(sourceColumns)\n",
    "val numDiffColumns = differenceColumns.size\n",
    "print(s\"Number of columns not matching the schema are: $numDiffColumns\")\n",
    "statsd.histogram(name = \"numDiffColumns\", value = numDiffColumns, tags = Seq(s\"appName:$appName\", \"data-validation\", \"env:dev\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom data quality checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility function to assert results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customCheck(val1 : Long, operator : String, threshold : Long) : Unit = {\n",
    "    operator match {\n",
    "        case \">\" => try { assert(val1 > threshold) } catch { case e: AssertionError => print(e);System.exit(1)}\n",
    "        case \">=\" => try { assert(val1 >= threshold) } catch { case e: AssertionError => print(e);System.exit(1)}\n",
    "        case \"=\" => try { assert(val1 == threshold) } catch { case e: AssertionError => print(e);System.exit(1)}\n",
    "        case \"<\" => try { assert(val1 < threshold) } catch { case e: AssertionError => print(e);System.exit(1)}\n",
    "        case \"<=\" => try { assert(val1 <= threshold) } catch { case e: AssertionError => print(e);System.exit(1)}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a temporary table, make sure that sql statements return a Long value, to be sure cast results to Long in the queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"dataset\")\n",
    "\n",
    "val res1 = spark.sql(customQ1).collect().toList(0).getAs[Long](0)\n",
    "customCheck(res1, customQ1Operator, customQ1ResultThreshold)\n",
    "\n",
    "val res2 = spark.sql(customQ2).collect().toList(0).getAs[Long](0)\n",
    "customCheck(res2, customQ2Operator, customQ2ResultThreshold)\n",
    "\n",
    "val res3 = spark.sql(customQ3).collect().toList(0).getAs[Long](0)\n",
    "customCheck(res3, customQ3Operator, customQ3ResultThreshold)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
