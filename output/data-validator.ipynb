{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016849,
     "end_time": "2019-03-31T06:34:01.534057",
     "exception": false,
     "start_time": "2019-03-31T06:34:01.517208",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Profiler and Schema Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.021525,
     "end_time": "2019-03-31T06:34:01.574574",
     "exception": false,
     "start_time": "2019-03-31T06:34:01.553049",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Profiles given input data based on the custom queries you provide, and validates its schema against schema repository. \n",
    "You can find how to insert a schema to schema-repository in README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "papermill": {
     "duration": 0.057776,
     "end_time": "2019-03-31T06:34:01.650639",
     "exception": false,
     "start_time": "2019-03-31T06:34:01.592863",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<table>\n",
       "  <tr>\n",
       "    <th>Magic</th>\n",
       "    <th>Example</th>\n",
       "    <th>Explanation</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>info</td>\n",
       "    <td>%%info</td>\n",
       "    <td>Outputs session information for the current Livy endpoint.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>cleanup</td>\n",
       "    <td>%%cleanup -f</td>\n",
       "    <td>Deletes all sessions for the current Livy endpoint, including this notebook's session. The force flag is mandatory.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>delete</td>\n",
       "    <td>%%delete -f -s 0</td>\n",
       "    <td>Deletes a session by number for the current Livy endpoint. Cannot delete this kernel's session.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>logs</td>\n",
       "    <td>%%logs</td>\n",
       "    <td>Outputs the current session's Livy logs.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>configure</td>\n",
       "    <td>%%configure -f<br/>{\"executorMemory\": \"1000M\", \"executorCores\": 4}</td>\n",
       "    <td>Configure the session creation parameters. The force flag is mandatory if a session has already been\n",
       "    created and the session will be dropped and recreated.<br/>Look at <a href=\"https://github.com/cloudera/livy#request-body\">\n",
       "    Livy's POST /sessions Request Body</a> for a list of valid parameters. Parameters must be passed in as a JSON string.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>spark</td>\n",
       "    <td>%%spark -o df<br/>df = spark.read.parquet('...</td>\n",
       "    <td>Executes spark commands.\n",
       "    Parameters:\n",
       "      <ul>\n",
       "        <li>-o VAR_NAME: The Spark dataframe of name VAR_NAME will be available in the %%local Python context as a\n",
       "          <a href=\"http://pandas.pydata.org/\">Pandas</a> dataframe with the same name.</li>\n",
       "        <li>-m METHOD: Sample method, either <tt>take</tt> or <tt>sample</tt>.</li>\n",
       "        <li>-n MAXROWS: The maximum number of rows of a dataframe that will be pulled from Livy to Jupyter.\n",
       "            If this number is negative, then the number of rows will be unlimited.</li>\n",
       "        <li>-r FRACTION: Fraction used for sampling.</li>\n",
       "      </ul>\n",
       "    </td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>sql</td>\n",
       "    <td>%%sql -o tables -q<br/>SHOW TABLES</td>\n",
       "    <td>Executes a SQL query against the variable sqlContext (Spark v1.x) or spark (Spark v2.x).\n",
       "    Parameters:\n",
       "      <ul>\n",
       "        <li>-o VAR_NAME: The result of the SQL query will be available in the %%local Python context as a\n",
       "          <a href=\"http://pandas.pydata.org/\">Pandas</a> dataframe.</li>\n",
       "        <li>-q: The magic will return None instead of the dataframe (no visualization).</li>\n",
       "        <li>-m, -n, -r are the same as the %%spark parameters above.</li>\n",
       "      </ul>\n",
       "    </td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>local</td>\n",
       "    <td>%%local<br/>a = 1</td>\n",
       "    <td>All the code in subsequent lines will be executed locally. Code must be valid Python code.</td>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.026591,
     "end_time": "2019-03-31T06:34:01.702115",
     "exception": false,
     "start_time": "2019-03-31T06:34:01.675524",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Spark job configuration parameters like memory and cores may vary from one job to other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "papermill": {
     "duration": 1.946674,
     "end_time": "2019-03-31T06:34:03.674598",
     "exception": false,
     "start_time": "2019-03-31T06:34:01.727924",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'name': 'data-profiler', 'executorMemory': '2GB', 'executorCores': 4, 'conf': {'spark.jars.packages': 'com.databricks:spark-avro_2.11:4.0.0,com.github.gphat:censorinus_2.11:2.1.13'}, 'kind': 'spark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\"name\":\"data-profiler\", \n",
    " \"executorMemory\": \"2GB\", \n",
    " \"executorCores\": 4, \n",
    " \"conf\": {\"spark.jars.packages\": \"com.databricks:spark-avro_2.11:4.0.0,com.github.gphat:censorinus_2.11:2.1.13\"} \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.028603,
     "end_time": "2019-03-31T06:34:03.721268",
     "exception": false,
     "start_time": "2019-03-31T06:34:03.692665",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Set parameters that will be overwritten by values passed externally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "papermill": {
     "duration": 35.798513,
     "end_time": "2019-03-31T06:34:39.550115",
     "exception": false,
     "start_time": "2019-03-31T06:34:03.751602",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>278</td><td>application_1550710474644_0438</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://sparkip:20888/proxy/application_1550710474644_0438/\">Link</a></td><td><a target=\"_blank\" href=\"http://sparkip:8042/node/containerlogs/container_1550710474644_0438_01_000001/livy\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataFormat: String = data-format\n",
      "delimiter: String = \"\"\n",
      "inputDataLocation: String = input-data-location\n",
      "appName: String = app-name\n",
      "schemaRepoUrl: String = schema-repo-url\n",
      "scheRepoSubjectName: String = subject-name\n",
      "schemaVersionId: String = schema-version\n",
      "customQ1: String = custom-query-1\n",
      "customQ1ResultThreshold: Int = 0\n",
      "customQ1Operator: String = custom-operator-1\n",
      "customQ2: String = custom-query-2\n",
      "customQ2ResultThreshold: Int = 0\n",
      "customQ2Operator: String = custom-operator-2\n",
      "customQ3: String = custom-query-3\n",
      "customQ3ResultThreshold: Int = 0\n",
      "customQ3Operator: String = custom-query-3\n"
     ]
    }
   ],
   "source": [
    "val dataFormat = \"data-format\"\n",
    "val delimiter = \"\"\n",
    "val inputDataLocation = \"input-data-location\"\n",
    "val appName = \"app-name\" \n",
    "val schemaRepoUrl = \"schema-repo-url\"\n",
    "val scheRepoSubjectName = \"subject-name\"\n",
    "val schemaVersionId = \"schema-version\"\n",
    "val customQ1 = \"custom-query-1\"\n",
    "val customQ1ResultThreshold = 0\n",
    "val customQ1Operator = \"custom-operator-1\"\n",
    "val customQ2 = \"custom-query-2\"\n",
    "val customQ2ResultThreshold = 0\n",
    "val customQ2Operator = \"custom-operator-2\"\n",
    "val customQ3 = \"custom-query-3\"\n",
    "val customQ3ResultThreshold = 0\n",
    "val customQ3Operator = \"custom-query-3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "papermill": {
     "duration": 3.268919,
     "end_time": "2019-03-31T06:34:42.838363",
     "exception": false,
     "start_time": "2019-03-31T06:34:39.569444",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataFormat: String = json\n",
      "inputDataLocation: String = s3a://bucket/prefix/generated.json\n",
      "appName: String = cust-profile-data-validation\n",
      "schemaRepoUrl: String = http://schemarepohostaddress\n",
      "scheRepoSubjectName: String = cust-profile\n",
      "schemaVersionId: String = 0\n",
      "customQ1: String = select CAST(count(_id) - count(distinct _id) as Long) as diff from dataset\n",
      "customQ1ResultThreshold: Int = 0\n",
      "customQ1Operator: String = =\n",
      "customQ2: String = select CAST(length(phone) as Long) from dataset\n",
      "customQ2ResultThreshold: Int = 17\n",
      "customQ2Operator: String = =\n",
      "customQ3: String = select CAST(count(distinct gender) as Long) from dataset\n",
      "customQ3ResultThreshold: Int = 3\n",
      "customQ3Operator: String = <=\n"
     ]
    }
   ],
   "source": [
    "// Parameters\n",
    "val dataFormat = \"json\"\n",
    "val inputDataLocation = \"s3a://bucket/prefix/generated.json\"\n",
    "val appName = \"cust-profile-data-validation\"\n",
    "val schemaRepoUrl = \"http://schemarepohostaddress\"\n",
    "val scheRepoSubjectName = \"cust-profile\"\n",
    "val schemaVersionId = \"0\"\n",
    "val customQ1 = \"select CAST(count(_id) - count(distinct _id) as Long) as diff from dataset\"\n",
    "val customQ1ResultThreshold = 0\n",
    "val customQ1Operator = \"=\"\n",
    "val customQ2 = \"select CAST(length(phone) as Long) from dataset\"\n",
    "val customQ2ResultThreshold = 17\n",
    "val customQ2Operator = \"=\"\n",
    "val customQ3 = \"select CAST(count(distinct gender) as Long) from dataset\"\n",
    "val customQ3ResultThreshold = 3\n",
    "val customQ3Operator = \"<=\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018232,
     "end_time": "2019-03-31T06:34:42.874800",
     "exception": false,
     "start_time": "2019-03-31T06:34:42.856568",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Setup datadog statsd interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "papermill": {
     "duration": 1.540569,
     "end_time": "2019-03-31T06:34:44.433086",
     "exception": false,
     "start_time": "2019-03-31T06:34:42.892517",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import github.gphat.censorinus.DogStatsDClient\n",
      "statsd: github.gphat.censorinus.DogStatsDClient = github.gphat.censorinus.DogStatsDClient@34f583f5\n"
     ]
    }
   ],
   "source": [
    "import github.gphat.censorinus.DogStatsDClient\n",
    "\n",
    "val statsd = new DogStatsDClient(hostname = \"localhost\", port = 8125, prefix = \"mlp.validator\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018379,
     "end_time": "2019-03-31T06:34:44.470167",
     "exception": false,
     "start_time": "2019-03-31T06:34:44.451788",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Read data, if data being read is CSV, it needs to have a header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "papermill": {
     "duration": 9.78802,
     "end_time": "2019-03-31T06:34:54.277048",
     "exception": false,
     "start_time": "2019-03-31T06:34:44.489028",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df: org.apache.spark.sql.DataFrame = [_id: string, about: string ... 20 more fields]\n"
     ]
    }
   ],
   "source": [
    "val df = dataFormat match {\n",
    "    case \"parquet\" => spark.read.parquet(inputDataLocation)\n",
    "    case \"json\" => spark.read.json(inputDataLocation)\n",
    "    case \"csv\" => spark.read.option(\"mode\", \"DROPMALFORMED\").option(\"header\", \"true\").option(\"delimiter\", delimiter).csv(inputDataLocation)\n",
    "    case _ => throw new Exception(s\"$dataFormat, as a dataformat is not supported \")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018455,
     "end_time": "2019-03-31T06:34:54.314874",
     "exception": false,
     "start_time": "2019-03-31T06:34:54.296419",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Publish some basic stats about the data. This can be extended further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "papermill": {
     "duration": 3.322344,
     "end_time": "2019-03-31T06:34:57.656056",
     "exception": false,
     "start_time": "2019-03-31T06:34:54.333712",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recordCount: Long = 7\n",
      "numColumns: Int = 22\n"
     ]
    }
   ],
   "source": [
    "val recordCount = df.count()\n",
    "val numColumns = df.columns.size\n",
    "statsd.histogram(name = \"recordCount\", value = recordCount, tags = Seq(s\"appName:$appName\", \"data-validation\", \"env:dev\"));\n",
    "statsd.histogram(name = \"numColumns\", value = numColumns, tags = Seq(s\"appName:$appName\", \"data-validation\",\"env:dev\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017359,
     "end_time": "2019-03-31T06:34:57.691555",
     "exception": false,
     "start_time": "2019-03-31T06:34:57.674196",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Read registered schema from schema repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.023315,
     "end_time": "2019-03-31T06:34:57.733195",
     "exception": false,
     "start_time": "2019-03-31T06:34:57.709880",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Utility method to call rest endpoint for schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "papermill": {
     "duration": 2.334079,
     "end_time": "2019-03-31T06:35:00.086641",
     "exception": false,
     "start_time": "2019-03-31T06:34:57.752562",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import java.io.IOException\n",
      "import org.apache.http.HttpEntity\n",
      "import org.apache.http.HttpResponse\n",
      "import org.apache.http.client.ClientProtocolException\n",
      "import org.apache.http.client.ResponseHandler\n",
      "import org.apache.http.client.methods.HttpGet\n",
      "import org.apache.http.impl.client.CloseableHttpClient\n",
      "import org.apache.http.impl.client.HttpClients\n",
      "import org.apache.http.util.EntityUtils\n",
      "getSchema: (url: String)String\n"
     ]
    }
   ],
   "source": [
    "import java.io.IOException;\n",
    "\n",
    "import org.apache.http.HttpEntity;\n",
    "import org.apache.http.HttpResponse;\n",
    "import org.apache.http.client.ClientProtocolException;\n",
    "import org.apache.http.client.ResponseHandler;\n",
    "import org.apache.http.client.methods.HttpGet;\n",
    "import org.apache.http.impl.client.CloseableHttpClient;\n",
    "import org.apache.http.impl.client.HttpClients;\n",
    "import org.apache.http.util.EntityUtils;\n",
    "\n",
    "def getSchema(url: String) : String = {\n",
    "    val httpclient: CloseableHttpClient = HttpClients.createDefault()\n",
    "    try {\n",
    "      val httpget: HttpGet = new HttpGet(url)\n",
    "      println(\"Executing request \" + httpget.getRequestLine)\n",
    "      val responseHandler: ResponseHandler[String] =\n",
    "        new ResponseHandler[String]() {\n",
    "          override def handleResponse(response: HttpResponse): String = {\n",
    "            var status: Int = response.getStatusLine.getStatusCode\n",
    "            if (status >= 200 && status < 300) {\n",
    "              var entity: HttpEntity = response.getEntity\n",
    "              if (entity != null) EntityUtils.toString(entity) else null\n",
    "            } else {\n",
    "              throw new ClientProtocolException(\n",
    "                \"Unexpected response status: \" + status);\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "       httpclient.execute(httpget, responseHandler)  \n",
    "    } finally {\n",
    "        httpclient.close()\n",
    "        None\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.019669,
     "end_time": "2019-03-31T06:35:00.126482",
     "exception": false,
     "start_time": "2019-03-31T06:35:00.106813",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Create url from input parameters and feth schema for specified version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "papermill": {
     "duration": 2.256894,
     "end_time": "2019-03-31T06:35:02.401766",
     "exception": false,
     "start_time": "2019-03-31T06:35:00.144872",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "schema_url: String = http://schemarepohostaddress/schema-repo/cust-profile/id/0\n",
      "Executing request GET http://schemarepohostaddress/schema-repo/cust-profile/id/0 HTTP/1.1\n",
      "publishedSchema: String =\n",
      "{\n",
      "  \"type\" : \"record\",\n",
      "  \"name\" : \"MyClass\",\n",
      "  \"namespace\" : \"com.test.avro\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"_id\",\n",
      "    \"type\" : \"string\"\n",
      "  }, {\n",
      "    \"name\" : \"index\",\n",
      "    \"type\" : \"long\"\n",
      "  }, {\n",
      "    \"name\" : \"guid\",\n",
      "    \"type\" : \"string\"\n",
      "  }, {\n",
      "    \"name\" : \"isActive\",\n",
      "    \"type\" : \"boolean\"\n",
      "  }, {\n",
      "    \"name\" : \"balance\",\n",
      "    \"type\" : \"string\"\n",
      "  }, {\n",
      "    \"name\" : \"picture\",\n",
      "    \"type\" : \"string\"\n",
      "  }, {\n",
      "    \"name\" : \"age\",\n",
      "    \"type\" : \"long\"\n",
      "  }, {\n",
      "    \"name\" : \"eyeColor\",\n",
      "    \"type\" : \"string\"\n",
      "  }, {\n",
      "    \"name\" : \"name\",\n",
      "    \"type\" : \"string\"\n",
      "  }, {\n",
      "    \"name\" : \"gender\",\n",
      "    \"type\" : \"string\"\n",
      "  }, {\n",
      "    \"name\" : \"company\",\n",
      "    \"type\" : \"string\"\n",
      "  }, {\n",
      "    \"name\" : \"email\",\n",
      "    \"type\" : \"string\"\n",
      "  }, {\n",
      "    \"name\" : \"phone\",\n",
      "    \"type\" : \"string\"\n",
      "  }, {\n",
      "    \"name..."
     ]
    }
   ],
   "source": [
    "val schema_url = s\"$schemaRepoUrl/schema-repo/$scheRepoSubjectName/id/$schemaVersionId\"\n",
    "val publishedSchema = getSchema(schema_url) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.019668,
     "end_time": "2019-03-31T06:35:02.441447",
     "exception": false,
     "start_time": "2019-03-31T06:35:02.421779",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Convert Avro schema registered to Spark SQL Schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "papermill": {
     "duration": 2.227334,
     "end_time": "2019-03-31T06:35:04.687955",
     "exception": false,
     "start_time": "2019-03-31T06:35:02.460621",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import com.databricks.spark.avro._\n",
      "import org.apache.avro.Schema.Parser\n",
      "schema: org.apache.avro.Schema = {\"type\":\"record\",\"name\":\"MyClass\",\"namespace\":\"com.test.avro\",\"fields\":[{\"name\":\"_id\",\"type\":\"string\"},{\"name\":\"index\",\"type\":\"long\"},{\"name\":\"guid\",\"type\":\"string\"},{\"name\":\"isActive\",\"type\":\"boolean\"},{\"name\":\"balance\",\"type\":\"string\"},{\"name\":\"picture\",\"type\":\"string\"},{\"name\":\"age\",\"type\":\"long\"},{\"name\":\"eyeColor\",\"type\":\"string\"},{\"name\":\"name\",\"type\":\"string\"},{\"name\":\"gender\",\"type\":\"string\"},{\"name\":\"company\",\"type\":\"string\"},{\"name\":\"email\",\"type\":\"string\"},{\"name\":\"phone\",\"type\":\"string\"},{\"name\":\"address\",\"type\":\"string\"},{\"name\":\"about\",\"type\":\"string\"},{\"name\":\"registered\",\"type\":\"string\"},{\"name\":\"latitude\",\"type\":\"double\"},{\"name\":\"longitude\",\"type\":\"double\"},{\"name\":\"tags\",\"type\":{\"type\":\"array\",\"items\":\"string\"}},{\"name\":\"friends\",\"type...import com.databricks.spark.avro.SchemaConverters\n",
      "structSchema: org.apache.spark.sql.types.DataType = StructType(StructField(_id,StringType,false), StructField(index,LongType,false), StructField(guid,StringType,false), StructField(isActive,BooleanType,false), StructField(balance,StringType,false), StructField(picture,StringType,false), StructField(age,LongType,false), StructField(eyeColor,StringType,false), StructField(name,StringType,false), StructField(gender,StringType,false), StructField(company,StringType,false), StructField(email,StringType,false), StructField(phone,StringType,false), StructField(address,StringType,false), StructField(about,StringType,false), StructField(registered,StringType,false), StructField(latitude,DoubleType,false), StructField(longitude,DoubleType,false), StructField(tags,ArrayType(StringType,false),false..."
     ]
    }
   ],
   "source": [
    "import com.databricks.spark.avro._\n",
    "import org.apache.avro.Schema.Parser\n",
    "val schema = new Parser().parse(publishedSchema)\n",
    "\n",
    "import com.databricks.spark.avro.SchemaConverters\n",
    "val structSchema =  SchemaConverters.toSqlType(schema).dataType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.019599,
     "end_time": "2019-03-31T06:35:04.727145",
     "exception": false,
     "start_time": "2019-03-31T06:35:04.707546",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Utility method to traverse schema tree and find the leaf node names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "papermill": {
     "duration": 2.286688,
     "end_time": "2019-03-31T06:35:07.033193",
     "exception": false,
     "start_time": "2019-03-31T06:35:04.746505",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import scala.collection.mutable.ListBuffer\n",
      "import org.apache.spark.sql.types._\n",
      "findFields: (path: String, dt: org.apache.spark.sql.types.DataType, columnNames: scala.collection.mutable.ListBuffer[String])Unit\n"
     ]
    }
   ],
   "source": [
    "import scala.collection.mutable.ListBuffer\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "def findFields(path: String, dt: DataType, columnNames: ListBuffer[String]): Unit = dt match {\n",
    "    case s: StructType =>\n",
    "      s.fields.foreach(f => findFields(path + \".\" + f.name, f.dataType, columnNames))\n",
    "    case s: ArrayType => findFields(path, s.elementType, columnNames)\n",
    "    case other =>\n",
    "      columnNames += path.substring(1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "papermill": {
     "duration": 2.299086,
     "end_time": "2019-03-31T06:35:09.350469",
     "exception": false,
     "start_time": "2019-03-31T06:35:07.051383",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dfColumnNames: scala.collection.mutable.ListBuffer[String] = ListBuffer()\n",
      "List(_id, about, address, age, balance, company, email, eyeColor, favoriteFruit, friends.id, friends.name, gender, greeting, guid, index, isActive, latitude, longitude, name, phone, picture, registered, tags)"
     ]
    }
   ],
   "source": [
    "var dfColumnNames = new ListBuffer[String]()\n",
    "findFields(\"\", df.schema, dfColumnNames)\n",
    "\n",
    "print(dfColumnNames.toList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "papermill": {
     "duration": 2.312576,
     "end_time": "2019-03-31T06:35:11.687080",
     "exception": false,
     "start_time": "2019-03-31T06:35:09.374504",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "publishedSchemaDataColumnNames: scala.collection.mutable.ListBuffer[String] = ListBuffer()\n",
      "List(_id, index, guid, isActive, balance, picture, age, eyeColor, name, gender, company, email, phone, address, about, registered, latitude, longitude, tags, friends.id, friends.name, greeting, favoriteFruit)"
     ]
    }
   ],
   "source": [
    "var publishedSchemaDataColumnNames = new ListBuffer[String]()\n",
    "findFields(\"\", structSchema, publishedSchemaDataColumnNames)\n",
    "\n",
    "print(publishedSchemaDataColumnNames.toList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "papermill": {
     "duration": 3.295563,
     "end_time": "2019-03-31T06:35:15.004001",
     "exception": false,
     "start_time": "2019-03-31T06:35:11.708438",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sourceColumns: scala.collection.immutable.Set[String] = Set(friends.id, registered, name, latitude, email, guid, _id, tags, balance, age, longitude, company, favoriteFruit, friends.name, isActive, greeting, address, picture, about, eyeColor, phone, index, gender)\n",
      "publishedColumns: scala.collection.immutable.Set[String] = Set(friends.id, registered, name, latitude, email, guid, _id, tags, balance, age, longitude, company, favoriteFruit, friends.name, isActive, greeting, address, picture, about, eyeColor, phone, index, gender)\n",
      "differenceColumns: scala.collection.immutable.Set[String] = Set()\n",
      "numDiffColumns: Int = 0\n",
      "Number of columns not matching the schema are: 0"
     ]
    }
   ],
   "source": [
    "val sourceColumns = dfColumnNames.toSet\n",
    "val publishedColumns = publishedSchemaDataColumnNames.toSet\n",
    "val differenceColumns = publishedColumns.diff(sourceColumns)\n",
    "val numDiffColumns = differenceColumns.size\n",
    "print(s\"Number of columns not matching the schema are: $numDiffColumns\")\n",
    "statsd.histogram(name = \"numDiffColumns\", value = numDiffColumns, tags = Seq(s\"appName:$appName\", \"data-validation\", \"env:dev\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.020864,
     "end_time": "2019-03-31T06:35:15.046792",
     "exception": false,
     "start_time": "2019-03-31T06:35:15.025928",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Custom data quality checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.029276,
     "end_time": "2019-03-31T06:35:15.096832",
     "exception": false,
     "start_time": "2019-03-31T06:35:15.067556",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Utility function to assert results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "papermill": {
     "duration": 1.565888,
     "end_time": "2019-03-31T06:35:16.684387",
     "exception": false,
     "start_time": "2019-03-31T06:35:15.118499",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customCheck: (val1: Long, operator: String, threshold: Long)Unit\n"
     ]
    }
   ],
   "source": [
    "def customCheck(val1 : Long, operator : String, threshold : Long) : Unit = {\n",
    "    operator match {\n",
    "        case \">\" => try { assert(val1 > threshold) } catch { case e: AssertionError => print(e);System.exit(1)}\n",
    "        case \">=\" => try { assert(val1 >= threshold) } catch { case e: AssertionError => print(e);System.exit(1)}\n",
    "        case \"=\" => try { assert(val1 == threshold) } catch { case e: AssertionError => print(e);System.exit(1)}\n",
    "        case \"<\" => try { assert(val1 < threshold) } catch { case e: AssertionError => print(e);System.exit(1)}\n",
    "        case \"<=\" => try { assert(val1 <= threshold) } catch { case e: AssertionError => print(e);System.exit(1)}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.021649,
     "end_time": "2019-03-31T06:35:16.727162",
     "exception": false,
     "start_time": "2019-03-31T06:35:16.705513",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Create a temporary table, make sure that sql statements return a Long value, to be sure cast results to Long in the queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "papermill": {
     "duration": 11.933366,
     "end_time": "2019-03-31T06:35:28.681401",
     "exception": false,
     "start_time": "2019-03-31T06:35:16.748035",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res1: Long = 0\n",
      "res2: Long = 17\n",
      "res3: Long = 1\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"dataset\")\n",
    "\n",
    "val res1 = spark.sql(customQ1).collect().toList(0).getAs[Long](0)\n",
    "customCheck(res1, customQ1Operator, customQ1ResultThreshold)\n",
    "\n",
    "val res2 = spark.sql(customQ2).collect().toList(0).getAs[Long](0)\n",
    "customCheck(res2, customQ2Operator, customQ2ResultThreshold)\n",
    "\n",
    "val res3 = spark.sql(customQ3).collect().toList(0).getAs[Long](0)\n",
    "customCheck(res3, customQ3Operator, customQ3ResultThreshold)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  },
  "papermill": {
   "duration": 90.051482,
   "end_time": "2019-03-31T06:35:30.080425",
   "environment_variables": {},
   "exception": null,
   "input_path": "data-validator.ipynb",
   "output_path": "output/data-validator.ipynb",
   "parameters": {
    "appName": "cust-profile-data-validation",
    "customQ1": "select CAST(count(_id) - count(distinct _id) as Long) as diff from dataset",
    "customQ1Operator": "=",
    "customQ1ResultThreshold": 0,
    "customQ2": "select CAST(length(phone) as Long) from dataset",
    "customQ2Operator": "=",
    "customQ2ResultThreshold": 17,
    "customQ3": "select CAST(count(distinct gender) as Long) from dataset",
    "customQ3Operator": "<=",
    "customQ3ResultThreshold": 3,
    "dataFormat": "json",
    "inputDataLocation": "s3a://ml-sort-summarize-clickstream-parquet/data-validation-test-dataset/generated.json",
    "scheRepoSubjectName": "cust-profile",
    "schemaRepoUrl": "http://schemarepohostaddress",
    "schemaVersionId": "0"
   },
   "start_time": "2019-03-31T06:34:00.028943",
   "version": "0+untagged.5.gec97e17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}